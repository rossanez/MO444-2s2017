{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MO444-A 2s/2017 - Second assignment\n",
    "#\n",
    "#         Group 05\n",
    "#\n",
    "# - Anderson Rossanez (124136)\n",
    "# - Bruno Branta Lopes (31470)\n",
    "#\n",
    "\n",
    "from __future__ import print_function\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "from scipy import misc\n",
    "from skimage.io import imread\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import norm\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Some definitions\n",
    "classes = np.asarray(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "int_classes = preprocessing.LabelEncoder().fit_transform(classes) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "onehot_classes = onehot_encoder.fit_transform(int_classes.reshape(len(int_classes), 1))\n",
    "\n",
    "def load_image_dataset(name, sample=1., as_gray=False):\n",
    "    \"\"\"Loads an image dataset\n",
    "    name: directory in which dataset is defined\n",
    "    sample: fraction of the dataset to be loaded [0, 1]\n",
    "    as_gray: read images as grayscale to have a 32x32 matrix, instead of a 32x32x3 matrix in case of RGB.\n",
    "    \"\"\"\n",
    "    random.seed(1)\n",
    "    X, Y = [], []\n",
    "    with open('cifar-10/%s/labels' % name) as labels:\n",
    "        i = 0\n",
    "        for path in sorted(glob.glob('cifar-10/%s/*.png' % name)):\n",
    "            y = int(labels.next())\n",
    "            if random.random() > sample:\n",
    "                continue\n",
    "            Y.append(y)\n",
    "            X.append(imread(path, as_grey=as_gray))\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    n_dim = reduce(lambda x, y: x * y, X.shape[1:])\n",
    "    return X.reshape(-1, n_dim), Y\n",
    "\n",
    "def display_image(x):\n",
    "    \"\"\"Prints a colored or grayscale 32x32 image\"\"\"\n",
    "    colors = x.shape[0] / 32 / 32\n",
    "    cmap = 'gray' if colors == 1 else 'jet'\n",
    "    new_shape = (32, 32) if colors == 1 else (32, 32, colors)\n",
    "    plt.imshow(x.reshape(new_shape), cmap=cmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load train dataset\n",
    "data_X, data_Y = load_image_dataset('train', as_gray=True, sample=0.1)\n",
    "print(\"Loaded %d samples\" % len(data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at a sample and its class\n",
    "display_image(data_X[207])\n",
    "\n",
    "print('Label: %s' % classes[int(data_Y[207])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementing K-Fold to help avoiding overfitting\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def print_results(k, start_time, train_precision, train_recall, train_f1, \n",
    "                  validation_precision, validation_recall, validation_f1):\n",
    "    print('\\nk={} time elapsed: {}'.format(k, datetime.now() - start_time))\n",
    "    print('           Precision  sd      Recall sd     F1 Score sd')\n",
    "    print('Training:      %5.2f  ±%5.2f   %5.2f ±%5.2f    %5.2f ±%5.2f' % (\n",
    "            np.mean(train_precision), np.std(train_precision), np.mean(train_recall), np.std(train_recall), np.mean(train_f1), np.std(train_f1)))\n",
    "    print('Validation:    %5.2f  ±%5.2f   %5.2f ±%5.2f    %5.2f ±%5.2f' % (\n",
    "            np.mean(validation_precision), np.std(validation_precision), np.mean(validation_recall), np.std(validation_recall), np.mean(validation_f1), np.std(validation_f1)))\n",
    "\n",
    "def run_kfold(method, data_Y, data_X, scale=False, check_gradients=False, report_timeout_seconds=60):\n",
    "    # prepare for 5-fold execution\n",
    "    random_state = np.random.RandomState(1)\n",
    "    k5_fold = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    train_precision, train_recall, train_f1 = [], [], []\n",
    "    validation_precision, validation_recall, validation_f1 = [], [], []\n",
    "    start_time = last_time = datetime.now()\n",
    "    k = 0\n",
    "    model = None\n",
    "\n",
    "    for train_index, validation_index in k5_fold.split(data_X):\n",
    "        k += 1\n",
    "        print('k%d' % k, end=' ')\n",
    "        \n",
    "        train_data_X, train_data_Y = data_X[train_index], data_Y[train_index]\n",
    "        validation_data_X, validation_data_Y = data_X[validation_index], data_Y[validation_index]\n",
    "        \n",
    "        if scale:\n",
    "            model_scaler = preprocessing.StandardScaler()\n",
    "            train_data_X = model_scaler.fit_transform(train_data_X)\n",
    "            validation_data_X = model_scaler.transform(validation_data_X)\n",
    "\n",
    "        # Train the model(s) using the training data\n",
    "        if check_gradients == True and k == 4:\n",
    "            model = method(train_data_X, train_data_Y, True)\n",
    "        elif check_gradients == True:\n",
    "            model = method(train_data_X, train_data_Y, False)\n",
    "        else:\n",
    "            model = method(train_data_X, train_data_Y)\n",
    "        \n",
    "        # Predict training data\n",
    "        predicted_train_data_Y = model.predict(train_data_X)\n",
    "        train_precision.append(precision_score(train_data_Y, predicted_train_data_Y, average='weighted'))\n",
    "        train_recall.append(recall_score(train_data_Y, predicted_train_data_Y, average='weighted'))\n",
    "        train_f1.append(f1_score(train_data_Y, predicted_train_data_Y, average='weighted'))\n",
    "        \n",
    "        # Predict validation data\n",
    "        predicted_validation_data_Y = model.predict(validation_data_X)\n",
    "        validation_precision.append(precision_score(validation_data_Y, predicted_validation_data_Y, average='weighted'))\n",
    "        validation_recall.append(recall_score(validation_data_Y, predicted_validation_data_Y, average='weighted'))\n",
    "        validation_f1.append(f1_score(validation_data_Y, predicted_validation_data_Y, average='weighted'))\n",
    "        if (datetime.now() - last_time).total_seconds() > report_timeout_seconds:\n",
    "            print_results(k, start_time, train_precision, train_recall, train_f1, validation_precision, validation_recall, validation_f1)\n",
    "            \n",
    "        last_time = datetime.now()\n",
    "    \n",
    "    print_results(k, start_time, train_precision, train_recall, train_f1, validation_precision, validation_recall, validation_f1)\n",
    "\n",
    "    try:\n",
    "        model.close()\n",
    "    except AttributeError:\n",
    "        pass # Nevermind...\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseline One-vs-All logistic regression model\n",
    "ova_lr_model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=100)\n",
    "run_kfold(lambda X, Y: ova_lr_model.fit(X, Y), data_Y, data_X, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Multinomial (Softmax) logistic regression model\n",
    "mn_lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100)\n",
    "run_kfold(lambda X, Y: mn_lr_model.fit(X, Y), data_Y, data_X, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_layers=[3], learning_rate=0.1, epochs=10):\n",
    "        tf.reset_default_graph() # To make sure everything is brand new.\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layers_neurons = []\n",
    "        self.layers_neurons.append(self.input_size)\n",
    "        self.layers_neurons += hidden_layers\n",
    "        self.layers_neurons.append(self.output_size)\n",
    "\n",
    "        self.num_layers = len(self.layers_neurons) - 1\n",
    "\n",
    "        self.thetas = {}\n",
    "        self.biases = {}\n",
    "        self.sums = {}\n",
    "        self.activations = {}\n",
    "\n",
    "        self.setup_nn()\n",
    "        self.setup_cost()\n",
    "\n",
    "        # define a tensorflow session and initialize the variables.\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def setup_nn(self):\n",
    "        # input and output placeholders\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.input_size], name='input')\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.output_size], name='output')\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.thetas[i] = tf.Variable(tf.random_normal([(self.input_size if i == 0 else self.layers_neurons[i]), self.layers_neurons[i+1]], stddev=0.03), name='W%d'%(i))\n",
    "            self.biases[i] = tf.Variable(tf.random_normal([self.layers_neurons[i+1]], stddev=0.03), name='b%d'%(i))\n",
    "\n",
    "            self.sums[i] = tf.add(tf.matmul(self.x if i == 0 else self.activations[i-1], self.thetas[i]), self.biases[i])\n",
    "            self.activations[i] = self.activation(self.sums[i])\n",
    "\n",
    "        # Feeding forward operation\n",
    "        self.y_pred = tf.nn.softmax(self.activations[i])\n",
    "        \n",
    "    def setup_cost(self):\n",
    "        # Cost function (cross entropy)\n",
    "        y_pred_clipped = tf.clip_by_value(self.y_pred, 1e-10, 0.9999999) # To avoid log(0), returning NaN\n",
    "        self.cost = -tf.reduce_mean(tf.reduce_sum(self.y * tf.log(y_pred_clipped) + (1 - self.y) * tf.log(1 - y_pred_clipped), axis=1))\n",
    "\n",
    "    def get_gradients(self, feed_dict):\n",
    "        diff_out = self.session.run(tf.subtract(self.y_pred, self.y), feed_dict)\n",
    "            \n",
    "        dThetas = {}\n",
    "        dBiases = {}\n",
    "        previous_delta = None\n",
    "        sums_index = len(self.sums) -1\n",
    "            \n",
    "        for i in xrange(self.num_layers-1, -1, -1):\n",
    "            if i == self.num_layers-1:\n",
    "                delta = self.session.run(\n",
    "                    tf.multiply(diff_out, self.activation_prime(self.sums[sums_index])), feed_dict)\n",
    "                dBiases[i] = delta\n",
    "                dThetas[i] = self.session.run(\n",
    "                    tf.matmul(tf.transpose(self.activations[sums_index-1]), delta), feed_dict)\n",
    "                previous_delta = delta\n",
    "            else:\n",
    "                delta = self.session.run(\n",
    "                    tf.multiply(tf.matmul(previous_delta, tf.transpose(self.thetas[sums_index+1])), self.activation_prime(self.sums[sums_index])), feed_dict)\n",
    "                dBiases[i] = delta\n",
    "                dThetas[i] = self.session.run(\n",
    "                    tf.matmul(tf.transpose(self.x if i == 0 else self.activations[sums_index-1]), delta), feed_dict)\n",
    "                previous_delta = delta\n",
    "            sums_index -= 1\n",
    "            \n",
    "        return dThetas, dBiases\n",
    "\n",
    "    def update_weights(self, dThetas, dBiases):\n",
    "        for i in range(self.num_layers):\n",
    "            step = [\n",
    "                tf.assign(self.thetas[i],\n",
    "                                 tf.subtract(self.thetas[i], tf.multiply(tf.constant(self.learning_rate), dThetas[i]))),\n",
    "                tf.assign(self.biases[i],\n",
    "                                 tf.subtract(self.biases[i], tf.multiply(tf.constant(self.learning_rate), tf.reduce_mean(dBiases[i], axis=[0]))))\n",
    "            ]\n",
    "            self.session.run(step)\n",
    "\n",
    "    def check_gradients(self, dThetas, dBiases, feed_dict):\n",
    "        print('Gradient checking (this will take a while!)...')\n",
    "        # TODO: implement it!\n",
    "        \n",
    "        all_grads = []\n",
    "        all_num_grads = []\n",
    "        for i in range(len(dThetas)):\n",
    "            w_tensor, b_tensor = self.thetas[i], self.biases[i]\n",
    "            gb = self.session.run(tf.reduce_mean(dBiases[i], axis=[0]))\n",
    "            gw = dThetas[i]\n",
    "            \n",
    "            #TODO: Find a way to flatten all the lists into a 1-D \"all_x\" list\n",
    "            #numgrad_w = self.get_numerical_grad(w_tensor, gw, feed_dict)\n",
    "            #all_num_grads += numgrad_w\n",
    "            #all_grads += gw\n",
    "            \n",
    "            #numgrad_b = self.get_numerical_grad(b_tensor, gb, feed_dict)\n",
    "            #all_num_grads += numgrad_b\n",
    "            #all_grads += gb\n",
    "        \n",
    "        return norm(all_grads - all_num_grads) / norm(all_grads + all_num_grads)\n",
    "            \n",
    "    def get_numerical_grad(self, var_tensor, grad, feed_dict):\n",
    "        original = self.session.run(var_tensor)\n",
    "        \n",
    "        numgrad = np.zeros(original.shape)\n",
    "        perturb = np.zeros(original.shape)\n",
    "        e = 1e-4\n",
    "\n",
    "        for p in range(len(original)):\n",
    "            #Set perturbation vector\n",
    "            perturb[p] = e\n",
    "            self.session.run(tf.assign(var_tensor, tf.add(original, perturb)))\n",
    "            loss2 = self.session.run(self.cost, feed_dict)\n",
    "            \n",
    "            self.session.run(tf.assign(var_tensor, tf.subtract(original, perturb)))\n",
    "            loss1 = loss2 = self.session.run(self.cost, feed_dict)\n",
    "\n",
    "            #Compute Numerical Gradient\n",
    "            numgrad[p] = (loss2 - loss1) / (2*e)\n",
    "\n",
    "            #Return the value we changed to zero:\n",
    "            perturb[p] = 0\n",
    "            \n",
    "        #Return Params to original value:\n",
    "        self.session.run(tf.assign(var_tensor, original))\n",
    "\n",
    "        return numgrad\n",
    "            \n",
    "\n",
    "    def activation(self, x):\n",
    "        return tf.div(tf.constant(1.0), tf.add(tf.constant(1.0), tf.exp(tf.negative(x))))\n",
    "    \n",
    "    def activation_prime(self, x):\n",
    "        return self.activation(x) * (1 - self.activation(x))\n",
    "    \n",
    "    def fit(self, X, Y, check_gradients=False):\n",
    "        # Apply One Hot Encoding\n",
    "        Y = onehot_encoder.transform(Y.reshape(len(Y), 1))\n",
    "        \n",
    "        costs = []\n",
    "        for epoch in range(self.epochs):\n",
    "            # Feed forward\n",
    "            self.session.run(self.y_pred, feed_dict={self.x: X, self.y: Y})\n",
    "\n",
    "            # Back propagation\n",
    "            dThetas, dBiases = self.get_gradients(feed_dict={self.x: X, self.y: Y})\n",
    "            if check_gradients == True and epoch == self.epochs - 1:\n",
    "                # check gradients\n",
    "                check_value = self.check_gradients(dThetas, dBiases, feed_dict={self.x: X, self.y: Y})\n",
    "                print('Gradient check result: {}'.format(check_value))\n",
    "                print('Is it arount 1e-8?')\n",
    "                \n",
    "            self.update_weights(dThetas, dBiases)\n",
    "\n",
    "            c = self.session.run(self.cost, feed_dict={self.x: X, self.y: Y})\n",
    "            #print(\"Epoch:\", (epoch + 1), \"cost: \", c)\n",
    "            costs.append(c)\n",
    "        \n",
    "        self.print_costs(costs)\n",
    "        return self\n",
    "\n",
    "    def print_costs(self, costs):\n",
    "        plt.plot(costs)\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward propagation\n",
    "        prediction = self.session.run(self.y_pred, feed_dict={self.x: X})\n",
    "        return np.argmax(prediction, 1)\n",
    "    \n",
    "    def close(self):\n",
    "        self.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Neural Network with a single hidden layer\n",
    "nn_single_hl_model = Neural_Network(input_size=len(data_X[0]), output_size=len(classes), hidden_layers=[1], learning_rate=0.01, epochs=2)\n",
    "run_kfold(lambda X, Y, check_gradients: nn_single_hl_model.fit(X, Y, check_gradients), data_Y, data_X, check_gradients=True, report_timeout_seconds=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network with two hidden layers\n",
    "nn_two_hl_model = Neural_Network(input_size=len(data_X[0]), output_size=len(classes), hidden_layers=[30, 30],  learning_rate=0.01, epochs=100)\n",
    "run_kfold(lambda X, Y: nn_two_hl_model.fit(X, Y), data_Y, data_X, report_timeout_seconds=120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
