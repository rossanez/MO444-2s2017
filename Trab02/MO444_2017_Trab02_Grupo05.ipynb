{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MO444-A 2s/2017 - Second assignment\n",
    "#\n",
    "#         Group 05\n",
    "#\n",
    "# - Anderson Rossanez (124136)\n",
    "# - Bruno Branta Lopes (31470)\n",
    "#\n",
    "\n",
    "from __future__ import print_function\n",
    "import glob\n",
    "import random\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "from skimage.io import imread\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Some definitions\n",
    "classes = np.asarray(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "int_classes = preprocessing.LabelEncoder().fit_transform(classes) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "onehot_classes = onehot_encoder.fit_transform(int_classes.reshape(len(int_classes), 1))\n",
    "\n",
    "def load_image_dataset(name, sample=1., as_gray=False):\n",
    "    \"\"\"Loads an image dataset\n",
    "    name: directory in which dataset is defined\n",
    "    sample: fraction of the dataset to be loaded [0, 1]\n",
    "    as_gray: read images as grayscale to have a 32x32 matrix, instead of a 32x32x3 matrix in case of RGB.\n",
    "    \"\"\"\n",
    "    random.seed(1)\n",
    "    X, Y = [], []\n",
    "    with open('cifar-10/%s/labels' % name) as labels:\n",
    "        i = 0\n",
    "        for path in sorted(glob.glob('cifar-10/%s/*.png' % name)):\n",
    "            y = int(labels.next())\n",
    "            if random.random() > sample:\n",
    "                continue\n",
    "            Y.append(y)\n",
    "            X.append(imread(path, as_grey=as_gray))\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    n_dim = reduce(lambda x, y: x * y, X.shape[1:])\n",
    "    return X.reshape(-1, n_dim), Y\n",
    "\n",
    "def display_image(x):\n",
    "    \"\"\"Prints a colored or grayscale 32x32 image\"\"\"\n",
    "    colors = x.shape[0] / 32 / 32\n",
    "    cmap = 'gray' if colors == 1 else 'jet'\n",
    "    new_shape = (32, 32) if colors == 1 else (32, 32, colors)\n",
    "    plt.imshow(x.reshape(new_shape), cmap=cmap)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load train dataset\n",
    "data_X, data_Y = load_image_dataset('train', as_gray=True, sample=0.1)\n",
    "print(\"Loaded %d samples\" % len(data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at a sample and its class\n",
    "display_image(data_X[207])\n",
    "\n",
    "print('Label: %s' % classes[int(data_Y[207])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementing K-Fold to help avoiding overfitting\n",
    "from sklearn.model_selection import KFold\n",
    "from datetime import datetime\n",
    "\n",
    "random_state = np.random.RandomState(1)\n",
    "\n",
    "# prepare for 5-fold execution\n",
    "k5_fold = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "def run_kfold(method, data_Y, data_X, scale=False):\n",
    "    train_precision, train_recall, train_f1 = [], [], []\n",
    "    validation_precision, validation_recall, validation_f1 = [], [], []\n",
    "    start_time = datetime.now()\n",
    "    k = 0\n",
    "    print('k', end=' ')\n",
    "    model = None\n",
    "\n",
    "    for train_index, validation_index in k5_fold.split(data_X):\n",
    "        k += 1\n",
    "        print(k, end=' ')\n",
    "        \n",
    "        train_data_X, train_data_Y = data_X[train_index], data_Y[train_index]\n",
    "        validation_data_X, validation_data_Y = data_X[validation_index], data_Y[validation_index]\n",
    "        \n",
    "        if scale:\n",
    "            model_scaler = preprocessing.StandardScaler()\n",
    "            train_data_X = model_scaler.fit_transform(train_data_X)\n",
    "            validation_data_X = model_scaler.transform(validation_data_X)\n",
    "\n",
    "        # Train the model(s) using the training data\n",
    "        model = method(train_data_X, train_data_Y)\n",
    "        \n",
    "        # Predict training data\n",
    "        predicted_train_data_Y = model.predict(train_data_X)\n",
    "        train_precision.append(precision_score(train_data_Y, predicted_train_data_Y, average='weighted'))\n",
    "        train_recall.append(recall_score(train_data_Y, predicted_train_data_Y, average='weighted'))\n",
    "        train_f1.append(f1_score(train_data_Y, predicted_train_data_Y, average='weighted'))\n",
    "        \n",
    "        # Predict validation data\n",
    "        predicted_validation_data_Y = model.predict(validation_data_X)\n",
    "        validation_precision.append(precision_score(validation_data_Y, predicted_validation_data_Y, average='weighted'))\n",
    "        validation_recall.append(recall_score(validation_data_Y, predicted_validation_data_Y, average='weighted'))\n",
    "        validation_f1.append(f1_score(validation_data_Y, predicted_validation_data_Y, average='weighted'))\n",
    "    \n",
    "    print('time elapsed: {}\\n'.format(datetime.now() - start_time))\n",
    "    print('           Precision  sd      Recall sd     F1 Score sd')\n",
    "    print('Training:      %5.2f  ±%5.2f   %5.2f ±%5.2f    %5.2f ±%5.2f' % (\n",
    "            np.mean(train_precision), np.std(train_precision), np.mean(train_recall), np.std(train_recall), np.mean(train_f1), np.std(train_f1)))\n",
    "    print('Validation:    %5.2f  ±%5.2f   %5.2f ±%5.2f    %5.2f ±%5.2f' % (\n",
    "            np.mean(validation_precision), np.std(validation_precision), np.mean(validation_recall), np.std(validation_recall), np.mean(validation_f1), np.std(validation_f1)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseline One-vs-All logistic regression model\n",
    "ova_lr_model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=100)\n",
    "run_kfold(lambda X, Y: ova_lr_model.fit(X, Y), data_Y, data_X, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Multinomial (Softmax) logistic regression model\n",
    "mn_lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100)\n",
    "run_kfold(lambda X, Y: mn_lr_model.fit(X, Y), data_Y, data_X, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural Networks\n",
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, learning_rate=0.5, epochs=10, batch_size=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # input data placeholders\n",
    "        self.x = tf.placeholder(tf.float32, [None, input_size])\n",
    "        # output data placeholder\n",
    "        self.y = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "        # now declare the weights connecting the input to the hidden layer\n",
    "        self.W1 = tf.Variable(tf.random_normal([input_size, 300], stddev=0.03), name='W1')\n",
    "        b1 = tf.Variable(tf.random_normal([300]), name='b1')\n",
    "\n",
    "        # and the weights connecting the hidden layer to the output layer\n",
    "        self.W2 = tf.Variable(tf.random_normal([300, output_size], stddev=0.03), name='W2')\n",
    "        b2 = tf.Variable(tf.random_normal([output_size]), name='b2')\n",
    "        \n",
    "        # hidden layer's output\n",
    "        Z1 = tf.add(tf.matmul(self.x, self.W1), b1)\n",
    "        a1 = self.activation(Z1)\n",
    "        \n",
    "        # output with softmax activation\n",
    "        Z2 = tf.add(tf.matmul(a1, self.W2), b2)\n",
    "        a2 = self.activation(Z2)\n",
    "        self.y_pred = tf.nn.softmax(a2)\n",
    "        \n",
    "        # Cost function\n",
    "        y_pred_clipped = tf.clip_by_value(self.y_pred, 1e-10, 0.9999999) # To avoid log(0), returning NaN\n",
    "        self.cost = -tf.reduce_mean(tf.reduce_sum(self.y * tf.log(y_pred_clipped) + (1 - self.y) * tf.log(1 - y_pred_clipped), axis=1))\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.cost)\n",
    "        \n",
    "        # Gradient checking\n",
    "        #self.gradients = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).compute_gradients(self.cost, self.get_trainable_variables())\n",
    "        \n",
    "        # finally setup the initialisation operator\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        \n",
    "        # define a tensorflow session\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(self.init_op)\n",
    "\n",
    "    def activation(self, x):\n",
    "        return tf.div(tf.constant(1.0), tf.add(tf.constant(1.0), tf.exp(tf.negative(x))))\n",
    "    \n",
    "    def activation_prime(self, x):\n",
    "        return tf.multiply(activation(x), tf.subtract(tf.constant(1.0), activation(x)))\n",
    "        \n",
    "    #def get_trainable_variables(self):\n",
    "        #return np.concatenate(self.W1, self.W2)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # Apply One Hot Encoding\n",
    "        Y = onehot_encoder.transform(Y.reshape(len(Y), 1))\n",
    "        \n",
    "        costs = []\n",
    "        # Training\n",
    "        for epoch in range(self.epochs):\n",
    "            _, c = self.session.run([self.optimizer, self.cost], feed_dict={self.x: X, self.y: Y})\n",
    "            # print(\"Epoch:\", (epoch + 1), \"cost: \", c)\n",
    "            costs.append(c)\n",
    "            #if epoch == 5:\n",
    "            #    gradients = self.session.run(self.gradients)\n",
    "\n",
    "        self.print_costs(costs)\n",
    "        return self\n",
    "    \n",
    "    def print_costs(self, costs):\n",
    "        plt.plot(costs)\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward propagation\n",
    "        prediction = self.session.run(self.y_pred, feed_dict={self.x: X})\n",
    "        return np.argmax(prediction, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network with a single hidden layer\n",
    "nn_single_hl_model = NeuralNetwork(input_size=len(data_X[0]), output_size=len(classes))\n",
    "run_kfold(lambda X, Y: nn_single_hl_model.fit(X, Y), data_Y, data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
