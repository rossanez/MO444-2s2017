{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/scipy/sparse/coo.py:200: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  if np.rank(self.data) != 1 or np.rank(self.row) != 1 or np.rank(self.col) != 1:\n",
      "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/scipy/sparse/compressed.py:130: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  if np.rank(self.data) != 1 or np.rank(self.indices) != 1 or np.rank(self.indptr) != 1:\n"
     ]
    }
   ],
   "source": [
    "# MO444-A 2s/2017 - Second assignment\n",
    "#\n",
    "#         Group 05\n",
    "#\n",
    "# - Anderson Rossanez (124136)\n",
    "# - Bruno Branta Lopes (31470)\n",
    "#\n",
    "\n",
    "from __future__ import print_function\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "from scipy import misc\n",
    "from skimage.io import imread\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import norm\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Some definitions\n",
    "classes = np.asarray(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "int_classes = preprocessing.LabelEncoder().fit_transform(classes) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "onehot_classes = onehot_encoder.fit_transform(int_classes.reshape(len(int_classes), 1))\n",
    "\n",
    "def load_image_dataset(name, sample=1., as_gray=False):\n",
    "    \"\"\"Loads an image dataset\n",
    "    name: directory in which dataset is defined\n",
    "    sample: fraction of the dataset to be loaded [0, 1]\n",
    "    as_gray: read images as grayscale to have a 32x32 matrix, instead of a 32x32x3 matrix in case of RGB.\n",
    "    \"\"\"\n",
    "    random.seed(1)\n",
    "    X, Y = [], []\n",
    "    with open('cifar-10/%s/labels' % name) as labels:\n",
    "        i = 0\n",
    "        for path in sorted(glob.glob('cifar-10/%s/*.png' % name)):\n",
    "            y = int(labels.next())\n",
    "            if random.random() > sample:\n",
    "                continue\n",
    "            Y.append(y)\n",
    "            X.append(imread(path, as_grey=as_gray))\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    n_dim = reduce(lambda x, y: x * y, X.shape[1:])\n",
    "    return X.reshape(-1, n_dim), Y\n",
    "\n",
    "def display_image(x):\n",
    "    \"\"\"Prints a colored or grayscale 32x32 image\"\"\"\n",
    "    colors = x.shape[0] / 32 / 32\n",
    "    cmap = 'gray' if colors == 1 else 'jet'\n",
    "    new_shape = (32, 32) if colors == 1 else (32, 32, colors)\n",
    "    plt.imshow(x.reshape(new_shape), cmap=cmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 samples\n"
     ]
    }
   ],
   "source": [
    "# Load train dataset\n",
    "data_X, data_Y = load_image_dataset('train', as_gray=True, sample=0.25)\n",
    "print(\"Loaded %d samples\" % len(data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmfJ/Lf1Jv4aGfSQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cf47050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: cat\n"
     ]
    }
   ],
   "source": [
    "# Take a look at a sample and its class\n",
    "display_image(data_X[207])\n",
    "\n",
    "print('Label: %s' % classes[int(data_Y[207])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementing K-Fold to help avoiding overfitting\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def print_results(k, start_time, train_accuracy, train_precision, train_recall, train_f1, \n",
    "                  validation_accuracy, validation_precision, validation_recall, validation_f1):\n",
    "    print('\\nk={} time elapsed: {}'.format(k, datetime.now() - start_time))\n",
    "    print('                Accuracy sd    Precision sd      Recall sd     F1 Score sd')\n",
    "    print('Training:      %5.2f  ±%5.2f   %5.2f  ±%5.2f   %5.2f ±%5.2f    %5.2f ±%5.2f' % (\n",
    "            np.mean(train_accuracy), np.std(train_accuracy), np.mean(train_precision), np.std(train_precision), np.mean(train_recall), np.std(train_recall), np.mean(train_f1), np.std(train_f1)))\n",
    "    print('Validation:    %5.2f  ±%5.2f   %5.2f  ±%5.2f   %5.2f ±%5.2f    %5.2f ±%5.2f' % (\n",
    "            np.mean(validation_accuracy), np.std(validation_accuracy), np.mean(validation_precision), np.std(validation_precision), np.mean(validation_recall), np.std(validation_recall), np.mean(validation_f1), np.std(validation_f1)))\n",
    "\n",
    "def run_kfold(method, data_Y, data_X, scale=False, report_timeout_seconds=60, one_fold_test=False):\n",
    "    # prepare for 5-fold execution\n",
    "    random_state = np.random.RandomState(1)\n",
    "    k5_fold = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    train_accuracy, train_precision, train_recall, train_f1 = [], [], [], []\n",
    "    validation_accuracy, validation_precision, validation_recall, validation_f1 = [], [], [], []\n",
    "    start_time = last_time = datetime.now()\n",
    "    k = 0\n",
    "    model = None\n",
    "\n",
    "    for train_index, validation_index in k5_fold.split(data_X):\n",
    "        k += 1\n",
    "        print('k%d' % k, end=' ')\n",
    "        \n",
    "        train_data_X, train_data_Y = data_X[train_index], data_Y[train_index]\n",
    "        validation_data_X, validation_data_Y = data_X[validation_index], data_Y[validation_index]\n",
    "        \n",
    "        if scale:\n",
    "            model_scaler = preprocessing.StandardScaler()\n",
    "            train_data_X = model_scaler.fit_transform(train_data_X)\n",
    "            validation_data_X = model_scaler.transform(validation_data_X)\n",
    "\n",
    "        # Train the model(s) using the training data\n",
    "        model = method(train_data_X, train_data_Y)\n",
    "        \n",
    "        # Predict training data\n",
    "        predicted_train_data_Y = model.predict(train_data_X)\n",
    "        train_accuracy.append(accuracy_score(train_data_Y, predicted_train_data_Y))\n",
    "        train_precision.append(precision_score(train_data_Y, predicted_train_data_Y, average='weighted'))\n",
    "        train_recall.append(recall_score(train_data_Y, predicted_train_data_Y, average='weighted'))\n",
    "        train_f1.append(f1_score(train_data_Y, predicted_train_data_Y, average='weighted'))\n",
    "        \n",
    "        # Predict validation data\n",
    "        predicted_validation_data_Y = model.predict(validation_data_X)\n",
    "        validation_accuracy.append(accuracy_score(validation_data_Y, predicted_validation_data_Y))\n",
    "        validation_precision.append(precision_score(validation_data_Y, predicted_validation_data_Y, average='weighted'))\n",
    "        validation_recall.append(recall_score(validation_data_Y, predicted_validation_data_Y, average='weighted'))\n",
    "        validation_f1.append(f1_score(validation_data_Y, predicted_validation_data_Y, average='weighted'))\n",
    "        if one_fold_test:\n",
    "            break\n",
    "        if (datetime.now() - last_time).total_seconds() > report_timeout_seconds:\n",
    "            print_results(k, start_time, train_accuracy, train_precision, train_recall, train_f1, validation_accuracy, validation_precision, validation_recall, validation_f1)\n",
    "            \n",
    "        last_time = datetime.now()\n",
    "    \n",
    "    print_results(k, start_time, train_accuracy, train_precision, train_recall, train_f1, validation_accuracy, validation_precision, validation_recall, validation_f1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-vs-All logistic regression\n",
    "class OvA_logistic_regression(object):\n",
    "    \n",
    "    def __init__(self, num_classes=len(classes), max_iter=10):\n",
    "        self.num_classes = num_classes\n",
    "        self.classifiers = []\n",
    "        \n",
    "        for i in xrange(self.num_classes):\n",
    "            self.classifiers.append(LogisticRegression(solver='lbfgs', max_iter=max_iter))\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        for i in xrange(self.num_classes):\n",
    "            y_curr_class = deepcopy(Y)\n",
    "            curr_class_i = y_curr_class == i\n",
    "            y_curr_class[curr_class_i] = 1\n",
    "            y_curr_class[~curr_class_i] = 0\n",
    "            loss = self.classifiers[i].fit(X, y_curr_class)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        num_classifiers = len(self.classifiers)\n",
    "        predictions = np.zeros((num_classifiers, len(X)))\n",
    "        for i in xrange(num_classifiers):\n",
    "            predictions[i] = self.classifiers[i].predict(X)\n",
    "            \n",
    "        final_pred = np.argmax(predictions, axis=0)\n",
    "\n",
    "        return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k1 \n",
      "k=1 time elapsed: 0:05:55.403978\n",
      "                Accuracy sd    Precision sd      Recall sd     F1 Score sd\n",
      "Training:       0.16  ± 0.00    0.55  ± 0.00    0.16 ± 0.00     0.12 ± 0.00\n",
      "Validation:     0.14  ± 0.00    0.35  ± 0.00    0.14 ± 0.00     0.10 ± 0.00\n",
      "k2 \n",
      "k=2 time elapsed: 0:11:48.605387\n",
      "                Accuracy sd    Precision sd      Recall sd     F1 Score sd\n",
      "Training:       0.16  ± 0.00    0.55  ± 0.01    0.16 ± 0.00     0.12 ± 0.00\n",
      "Validation:     0.14  ± 0.00    0.37  ± 0.02    0.14 ± 0.00     0.10 ± 0.00\n",
      "k3 \n",
      "k=3 time elapsed: 0:17:42.227594\n",
      "                Accuracy sd    Precision sd      Recall sd     F1 Score sd\n",
      "Training:       0.16  ± 0.00    0.56  ± 0.01    0.16 ± 0.00     0.12 ± 0.00\n",
      "Validation:     0.14  ± 0.00    0.36  ± 0.02    0.14 ± 0.00     0.10 ± 0.00\n",
      "k4 \n",
      "k=4 time elapsed: 0:23:48.361097\n",
      "                Accuracy sd    Precision sd      Recall sd     F1 Score sd\n",
      "Training:       0.16  ± 0.00    0.56  ± 0.01    0.16 ± 0.00     0.12 ± 0.00\n",
      "Validation:     0.14  ± 0.00    0.36  ± 0.02    0.14 ± 0.00     0.10 ± 0.00\n",
      "k5 \n",
      "k=5 time elapsed: 0:29:49.859985\n",
      "                Accuracy sd    Precision sd      Recall sd     F1 Score sd\n",
      "Training:       0.16  ± 0.00    0.55  ± 0.01    0.16 ± 0.00     0.12 ± 0.00\n",
      "Validation:     0.14  ± 0.00    0.35  ± 0.02    0.14 ± 0.00     0.10 ± 0.00\n",
      "\n",
      "k=5 time elapsed: 0:29:49.861657\n",
      "                Accuracy sd    Precision sd      Recall sd     F1 Score sd\n",
      "Training:       0.16  ± 0.00    0.55  ± 0.01    0.16 ± 0.00     0.12 ± 0.00\n",
      "Validation:     0.14  ± 0.00    0.35  ± 0.02    0.14 ± 0.00     0.10 ± 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.OvA_logistic_regression at 0x11138c590>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a baseline One-vs-All logistic regression model\n",
    "ova_lr_model = OvA_logistic_regression(max_iter=1000)\n",
    "run_kfold(lambda X, Y: ova_lr_model.fit(X, Y), data_Y, data_X, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k1 \n",
      "k=1 time elapsed: 0:01:33.861376\n",
      "                Accuracy sd    Precision sd      Recall sd     F1 Score sd\n",
      "Training:       0.35  ± 0.00    0.35  ± 0.00    0.35 ± 0.00     0.35 ± 0.00\n",
      "Validation:     0.27  ± 0.00    0.26  ± 0.00    0.27 ± 0.00     0.26 ± 0.00\n",
      "k2 \n",
      "k=2 time elapsed: 0:03:06.771105\n",
      "                Accuracy sd    Precision sd      Recall sd     F1 Score sd\n",
      "Training:       0.35  ± 0.00    0.35  ± 0.00    0.35 ± 0.00     0.35 ± 0.00\n",
      "Validation:     0.27  ± 0.00    0.27  ± 0.00    0.27 ± 0.00     0.27 ± 0.00\n",
      "k3 \n",
      "k=3 time elapsed: 0:04:39.171198\n",
      "                Accuracy sd    Precision sd      Recall sd     F1 Score sd\n",
      "Training:       0.36  ± 0.00    0.35  ± 0.00    0.36 ± 0.00     0.35 ± 0.00\n",
      "Validation:     0.27  ± 0.00    0.27  ± 0.00    0.27 ± 0.00     0.27 ± 0.00\n",
      "k4 \n",
      "k=4 time elapsed: 0:06:13.156896\n",
      "                Accuracy sd    Precision sd      Recall sd     F1 Score sd\n",
      "Training:       0.36  ± 0.00    0.35  ± 0.00    0.36 ± 0.00     0.35 ± 0.00\n",
      "Validation:     0.27  ± 0.01    0.26  ± 0.01    0.27 ± 0.01     0.27 ± 0.01\n",
      "k5 \n",
      "k=5 time elapsed: 0:07:48.457967\n",
      "                Accuracy sd    Precision sd      Recall sd     F1 Score sd\n",
      "Training:       0.36  ± 0.00    0.35  ± 0.00    0.36 ± 0.00     0.35 ± 0.00\n",
      "Validation:     0.27  ± 0.01    0.26  ± 0.01    0.27 ± 0.01     0.26 ± 0.01\n",
      "\n",
      "k=5 time elapsed: 0:07:48.460462\n",
      "                Accuracy sd    Precision sd      Recall sd     F1 Score sd\n",
      "Training:       0.36  ± 0.00    0.35  ± 0.00    0.36 ± 0.00     0.35 ± 0.00\n",
      "Validation:     0.27  ± 0.01    0.26  ± 0.01    0.27 ± 0.01     0.26 ± 0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Multinomial (Softmax) logistic regression model\n",
    "mn_lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000)\n",
    "run_kfold(lambda X, Y: mn_lr_model.fit(X, Y), data_Y, data_X, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_layers=[3], learning_rate=0.1, epochs=10, \n",
    "                 activation_method='sigmoid', gradient_check=False):\n",
    "        self.batch_size=10\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation_method = activation_method\n",
    "        self.gradient_check = gradient_check\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layers_neurons = []\n",
    "        self.layers_neurons.append(self.input_size)\n",
    "        self.layers_neurons += hidden_layers\n",
    "        self.layers_neurons.append(self.output_size)\n",
    "\n",
    "        self.num_layers = len(self.layers_neurons) - 1\n",
    "\n",
    "        self.thetas = {}\n",
    "        self.biases = {}\n",
    "        self.sums = {}\n",
    "        self.activations = {}\n",
    "\n",
    "        # Define this model's graph and enter its context\n",
    "        self.our_graph = tf.Graph()\n",
    "        self.set_our_graph_context()\n",
    "        \n",
    "        self.setup_nn()\n",
    "        self.setup_cost()\n",
    "        self.setup_optimizer()\n",
    "        \n",
    "        # define a tensorflow session and initialize the variables\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def set_our_graph_context(self):\n",
    "        self.our_graph_context = self.our_graph.as_default()\n",
    "        self.our_graph_context.enforce_nesting = False\n",
    "        self.our_graph_context.__enter__()\n",
    "\n",
    "    def setup_nn(self):\n",
    "        # input and output placeholders\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.input_size], name='input')\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.output_size], name='output')\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.thetas[i] = tf.Variable(tf.random_normal([self.layers_neurons[i], self.layers_neurons[i+1]], stddev=0.03), name='W%d'%(i))\n",
    "            self.biases[i] = tf.Variable(tf.random_normal([self.layers_neurons[i+1]], stddev=0.03), name='b%d'%(i))\n",
    "\n",
    "            self.sums[i] = tf.add(tf.matmul(self.x if i == 0 else self.activations[i-1], self.thetas[i]), self.biases[i])\n",
    "            self.activations[i] = self.activation(self.sums[i])\n",
    "\n",
    "        # Feeding forward operation with softmax at the end\n",
    "        self.y_pred = tf.nn.softmax(self.activations[self.num_layers-1])\n",
    "        \n",
    "    def activation(self, x):\n",
    "        if self.activation_method == 'sigmoid':\n",
    "            return tf.div(tf.constant(1.0), tf.add(tf.constant(1.0), tf.exp(tf.negative(x))))\n",
    "        if self.activation_method == 'tanh':\n",
    "            return tf.tanh(x)\n",
    "        if self.activation_method == 'relu':\n",
    "            return tf.nn.relu(x)\n",
    "        if self.activation_method == 'softplus':\n",
    "            return tf.nn.softplus(x)\n",
    "        if self.activation_method == 'softsign':\n",
    "            return tf.nn.softsign(x)\n",
    "        raise Exception('Invalid activation method: ' + repr(self.activation_method))\n",
    "    \n",
    "    def setup_cost(self):\n",
    "        # Cost function (cross entropy)\n",
    "        y_pred_clipped = tf.clip_by_value(self.y_pred, 1e-10, 0.9999999) # To avoid log(0), returning NaN\n",
    "        self.cost = tf.reduce_mean(-tf.reduce_sum(self.y * tf.log(y_pred_clipped) + (1 - self.y) * tf.log(1 - y_pred_clipped), axis=1))\n",
    "\n",
    "    def setup_optimizer(self):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        self.train = optimizer.minimize(self.cost)\n",
    "\n",
    "    def check_gradients(self, feed_dict):\n",
    "        all_grads = []\n",
    "        all_num_grads = []\n",
    "        for i in range(self.num_layers):\n",
    "            w_tensor, b_tensor = self.thetas[i], self.biases[i]\n",
    "\n",
    "            numgrads_w, grads_w = self.get_numerical_grad(w_tensor, feed_dict)\n",
    "            all_num_grads += numgrads_w.tolist()\n",
    "            all_grads += grads_w.tolist()\n",
    "            \n",
    "            numgrads_b, grads_b = self.get_numerical_grad(b_tensor, feed_dict)\n",
    "            all_num_grads += numgrads_b.tolist()\n",
    "            all_grads += grads_b.tolist()\n",
    "        \n",
    "        all_num_grads = np.asarray(all_num_grads)\n",
    "        all_grads = np.asarray(all_grads)\n",
    "        \n",
    "        # Print a graph to compare original and calculated gradients\n",
    "        plt.plot(all_num_grads, label='num')\n",
    "        plt.plot(all_grads, label='orig')\n",
    "        plt.ylabel('thethas & biases')\n",
    "        plt.xlabel('grads')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        return norm(all_grads - all_num_grads) / norm(all_grads + all_num_grads)\n",
    "            \n",
    "    def get_numerical_grad(self, var_tensor, feed_dict):\n",
    "        grads = np.asarray(self.session.run(tf.gradients(self.cost, var_tensor), feed_dict)).ravel()\n",
    "        original = self.session.run(var_tensor)\n",
    "        \n",
    "        # a flattened view of the original (change it, and you'll change the original)\n",
    "        flattened = original.ravel()\n",
    "        e = 1e-4\n",
    "\n",
    "        num_grads = np.zeros(flattened.shape)\n",
    "        for p in range(len(flattened)):\n",
    "            # Do some perturbing\n",
    "            original_flat_p = flattened[p]\n",
    "            flattened[p] = original_flat_p + e\n",
    "            self.session.run(tf.assign(var_tensor, original))\n",
    "            loss2 = self.session.run(self.cost, feed_dict)\n",
    "\n",
    "            flattened[p] = original_flat_p - e\n",
    "            self.session.run(tf.assign(var_tensor, original))\n",
    "            loss1 = self.session.run(self.cost, feed_dict)\n",
    "\n",
    "            # Restore to original value\n",
    "            flattened[p] = original_flat_p\n",
    "            \n",
    "            # Compute Numerical Gradient\n",
    "            num_grads[p] = (loss2 - loss1) / (2 * e)\n",
    "            # print('Loss 2:', loss2, 'Loss 1:', loss1, 'Num Grad:', num_grads[p], 'Grad:', grads[p]) # debug\n",
    "\n",
    "        self.session.run(tf.assign(var_tensor, original))\n",
    "        return num_grads, grads\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.set_our_graph_context() # Using this model's own graph.\n",
    "\n",
    "        # Apply One Hot Encoding\n",
    "        Y = onehot_encoder.transform(Y.reshape(len(Y), 1))\n",
    "\n",
    "        costs = []\n",
    "        ran_check = False\n",
    "        for epoch in range(self.epochs):\n",
    "            avg_cost = 0\n",
    "            \n",
    "            if self.gradient_check and epoch == self.epochs - 1 and ran_check == False:\n",
    "                print('Gradient checking (this will take a while...)')\n",
    "                start_time = datetime.now()\n",
    "                check_value = self.check_gradients(feed_dict={self.x: X, self.y: Y})\n",
    "                print('Time elapsed: {}. Gradient check result: {}'.format(datetime.now() - start_time, check_value))\n",
    "                print('Is it around 1e-8?')\n",
    "                ran_check = True\n",
    "            else:\n",
    "                for i in range(self.batch_size):\n",
    "                    range_start = i * self.batch_size\n",
    "                    range_end = (i + 1) * self.batch_size\n",
    "                    feed_dict = {self.x: X[range_start:range_end], self.y: Y[range_start:range_end]}\n",
    "                    \n",
    "                    _, c = self.session.run([self.train, self.cost], feed_dict)\n",
    "                    avg_cost += c / self.batch_size\n",
    "                costs.append(avg_cost)\n",
    "\n",
    "        self.print_costs(costs)\n",
    "        return self\n",
    "\n",
    "    def print_costs(self, costs):\n",
    "        plt.plot(costs)\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.set_our_graph_context() # Using this model's own graph.\n",
    "\n",
    "        # Forward propagation\n",
    "        prediction = self.session.run(self.y_pred, feed_dict={self.x: X})\n",
    "        return np.argmax(prediction, 1)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.set_our_graph_context()\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.session, 'saved/' + name + '.nn.ckpt')\n",
    "    \n",
    "    def restore(self, name):\n",
    "        self.set_our_graph_context()\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.session, 'saved/' + name + '.nn.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simple neural network with a single hidden layer and a single neuron in it - to perform gradient checking\n",
    "gc_data_X, gc_data_Y = [], []\n",
    "num_class_types_in_sample = 2\n",
    "for y, klass in enumerate(classes):\n",
    "    index = np.flatnonzero(data_Y == y)\n",
    "    index = np.random.choice(index, num_class_types_in_sample, replace=False)\n",
    "    gc_data_X += data_X[index].tolist()\n",
    "    gc_data_Y += data_Y[index].tolist()\n",
    "\n",
    "nn_single_hl_model_one_neuron = Neural_Network(input_size=len(data_X[0]), output_size=len(classes), hidden_layers=[1], \n",
    "                                               learning_rate=0.01, gradient_check=True, epochs=10, activation_method='tanh')\n",
    "run_kfold(lambda X, Y: nn_single_hl_model_one_neuron.fit(X, Y), np.asarray(gc_data_Y), np.asarray(gc_data_X), one_fold_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activation_method in ['sigmoid', 'tanh', 'relu', 'softplus', 'softsign']:\n",
    "    print('\\n* Testing activation method:', activation_method)\n",
    "    nn_model = Neural_Network(input_size=len(data_X[0]), output_size=len(classes), \n",
    "                              hidden_layers=[32], learning_rate=.1, epochs=100,\n",
    "                              activation_method=activation_method)\n",
    "    run_kfold(lambda X, Y: nn_model.fit(X, Y), data_Y, data_X, one_fold_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network with a single hidden layer\n",
    "nn_single_hl_model = Neural_Network(input_size=len(data_X[0]), output_size=len(classes), hidden_layers=[1024],  learning_rate=0.01, epochs=1000)\n",
    "run_kfold(lambda X, Y: nn_single_hl_model.fit(X, Y), data_Y, data_X, report_timeout_seconds=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network with two hidden layers\n",
    "nn_two_hl_model = Neural_Network(input_size=len(data_X[0]), output_size=len(classes), hidden_layers=[30, 30],  learning_rate=0.01, epochs=100)\n",
    "run_kfold(lambda X, Y: nn_two_hl_model.fit(X, Y), data_Y, data_X, report_timeout_seconds=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: carry on with the testing (change activation functions, num_layers, num_neurons, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset (untouched so far)\n",
    "test_X, test_Y = load_image_dataset('test', as_gray=True)\n",
    "print(\"Loaded %d samples\" % len(data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test data using our chosen model\n",
    "# TODO: change to the actually elected model!!!!!!!!!!!!!!\n",
    "elected_model = nn_single_hl_model\n",
    "pred_test_Y = elected_model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion_matrix(test_Y, pred_test_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
