{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MO444-A 2s/2017 - Second assignment\n",
    "#\n",
    "#         Group 05\n",
    "#\n",
    "# - Anderson Rossanez (124136)\n",
    "# - Bruno Branta Lopes (31470)\n",
    "#\n",
    "\n",
    "from __future__ import print_function\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "from scipy import misc\n",
    "from skimage.io import imread\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Some definitions\n",
    "classes = np.asarray(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "int_classes = preprocessing.LabelEncoder().fit_transform(classes) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "onehot_classes = onehot_encoder.fit_transform(int_classes.reshape(len(int_classes), 1))\n",
    "\n",
    "def load_image_dataset(name, sample=1., as_gray=False):\n",
    "    \"\"\"Loads an image dataset\n",
    "    name: directory in which dataset is defined\n",
    "    sample: fraction of the dataset to be loaded [0, 1]\n",
    "    as_gray: read images as grayscale to have a 32x32 matrix, instead of a 32x32x3 matrix in case of RGB.\n",
    "    \"\"\"\n",
    "    random.seed(1)\n",
    "    X, Y = [], []\n",
    "    with open('cifar-10/%s/labels' % name) as labels:\n",
    "        i = 0\n",
    "        for path in sorted(glob.glob('cifar-10/%s/*.png' % name)):\n",
    "            y = int(labels.next())\n",
    "            if random.random() > sample:\n",
    "                continue\n",
    "            Y.append(y)\n",
    "            X.append(imread(path, as_grey=as_gray))\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    n_dim = reduce(lambda x, y: x * y, X.shape[1:])\n",
    "    return X.reshape(-1, n_dim), Y\n",
    "\n",
    "def display_image(x):\n",
    "    \"\"\"Prints a colored or grayscale 32x32 image\"\"\"\n",
    "    colors = x.shape[0] / 32 / 32\n",
    "    cmap = 'gray' if colors == 1 else 'jet'\n",
    "    new_shape = (32, 32) if colors == 1 else (32, 32, colors)\n",
    "    plt.imshow(x.reshape(new_shape), cmap=cmap)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load train dataset\n",
    "data_X, data_Y = load_image_dataset('train', as_gray=True, sample=0.1)\n",
    "print(\"Loaded %d samples\" % len(data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at a sample and its class\n",
    "display_image(data_X[207])\n",
    "\n",
    "print('Label: %s' % classes[int(data_Y[207])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementing K-Fold to help avoiding overfitting\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def print_results(k, start_time, train_precision, train_recall, train_f1, \n",
    "                  validation_precision, validation_recall, validation_f1):\n",
    "    print('\\nk={} time elapsed: {}'.format(k, datetime.now() - start_time))\n",
    "    print('           Precision  sd      Recall sd     F1 Score sd')\n",
    "    print('Training:      %5.2f  ±%5.2f   %5.2f ±%5.2f    %5.2f ±%5.2f' % (\n",
    "            np.mean(train_precision), np.std(train_precision), np.mean(train_recall), np.std(train_recall), np.mean(train_f1), np.std(train_f1)))\n",
    "    print('Validation:    %5.2f  ±%5.2f   %5.2f ±%5.2f    %5.2f ±%5.2f' % (\n",
    "            np.mean(validation_precision), np.std(validation_precision), np.mean(validation_recall), np.std(validation_recall), np.mean(validation_f1), np.std(validation_f1)))\n",
    "\n",
    "def run_kfold(method, data_Y, data_X, scale=False, check_gradients=False, report_timeout_seconds=60):\n",
    "    # prepare for 5-fold execution\n",
    "    random_state = np.random.RandomState(1)\n",
    "    k5_fold = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    train_precision, train_recall, train_f1 = [], [], []\n",
    "    validation_precision, validation_recall, validation_f1 = [], [], []\n",
    "    start_time = last_time = datetime.now()\n",
    "    k = 0\n",
    "    model = None\n",
    "\n",
    "    for train_index, validation_index in k5_fold.split(data_X):\n",
    "        k += 1\n",
    "        print('k%d' % k, end=' ')\n",
    "        \n",
    "        train_data_X, train_data_Y = data_X[train_index], data_Y[train_index]\n",
    "        validation_data_X, validation_data_Y = data_X[validation_index], data_Y[validation_index]\n",
    "        \n",
    "        if scale:\n",
    "            model_scaler = preprocessing.StandardScaler()\n",
    "            train_data_X = model_scaler.fit_transform(train_data_X)\n",
    "            validation_data_X = model_scaler.transform(validation_data_X)\n",
    "\n",
    "        # Train the model(s) using the training data\n",
    "        if check_gradients == True:\n",
    "            if k == 4:\n",
    "                model = method(train_data_X, train_data_Y, True)\n",
    "            else:\n",
    "                model = method(train_data_X, train_data_Y, False)\n",
    "        else:\n",
    "            model = method(train_data_X, train_data_Y)\n",
    "        \n",
    "        # Predict training data\n",
    "        predicted_train_data_Y = model.predict(train_data_X)\n",
    "        train_precision.append(precision_score(train_data_Y, predicted_train_data_Y, average='weighted'))\n",
    "        train_recall.append(recall_score(train_data_Y, predicted_train_data_Y, average='weighted'))\n",
    "        train_f1.append(f1_score(train_data_Y, predicted_train_data_Y, average='weighted'))\n",
    "        \n",
    "        # Predict validation data\n",
    "        predicted_validation_data_Y = model.predict(validation_data_X)\n",
    "        validation_precision.append(precision_score(validation_data_Y, predicted_validation_data_Y, average='weighted'))\n",
    "        validation_recall.append(recall_score(validation_data_Y, predicted_validation_data_Y, average='weighted'))\n",
    "        validation_f1.append(f1_score(validation_data_Y, predicted_validation_data_Y, average='weighted'))\n",
    "        if (datetime.now() - last_time).total_seconds() > report_timeout_seconds:\n",
    "            print_results(k, start_time, train_precision, train_recall, train_f1, validation_precision, validation_recall, validation_f1)\n",
    "            \n",
    "        last_time = datetime.now()\n",
    "    \n",
    "    print_results(k, start_time, train_precision, train_recall, train_f1, validation_precision, validation_recall, validation_f1)\n",
    "\n",
    "    try:\n",
    "        model.close()\n",
    "    except AttributeError:\n",
    "        pass # Nevermind...\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseline One-vs-All logistic regression model\n",
    "ova_lr_model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=100)\n",
    "run_kfold(lambda X, Y: ova_lr_model.fit(X, Y), data_Y, data_X, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Multinomial (Softmax) logistic regression model\n",
    "mn_lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100)\n",
    "run_kfold(lambda X, Y: mn_lr_model.fit(X, Y), data_Y, data_X, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Networks\n",
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_layers=[1], learning_rate=0.5, epochs=10):\n",
    "        tf.reset_default_graph() # To make sure everything is brand new.\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # input data placeholders\n",
    "        self.x = tf.placeholder(tf.float32, [None, input_size], name='input')\n",
    "        # output data placeholder\n",
    "        self.y = tf.placeholder(tf.float32, [None, output_size], name='output')\n",
    "\n",
    "        W = {} # hidden layers' inputs weights\n",
    "        b = {} # hidden layers' bias\n",
    "        Z = {} # hidden layers' neurons outputs\n",
    "        a = {} # hidden layers' activation\n",
    "        last_layer_size = input_size\n",
    "        for i, layer_size in enumerate(hidden_layers + [output_size]):\n",
    "            W[i] = tf.Variable(tf.random_normal([last_layer_size, layer_size], stddev=0.03), name='W%d'%(i+1))\n",
    "            b[i] = tf.Variable(tf.random_normal([layer_size]), name='b%d'%(i+1))\n",
    "            Z[i] = tf.add(tf.matmul(self.x if i == 0 else a[i-1], W[i]), b[i])\n",
    "            a[i] = self.activation(Z[i])\n",
    "            last_layer_size = layer_size\n",
    "\n",
    "        self.y_pred = tf.nn.softmax(a[i])\n",
    "        \n",
    "        # Cost function\n",
    "        y_pred_clipped = tf.clip_by_value(self.y_pred, 1e-10, 0.9999999) # To avoid log(0), returning NaN\n",
    "        self.cost = -tf.reduce_mean(tf.reduce_sum(self.y * tf.log(y_pred_clipped) + (1 - self.y) * tf.log(1 - y_pred_clipped), axis=1))\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # Optimize directly\n",
    "        self.optimization = optimizer.minimize(self.cost)\n",
    "        \n",
    "        # To perform gradient checking (minimize is the same as compute_gradients + apply_gradients)\n",
    "        self.get_gradients = optimizer.compute_gradients(self.cost)\n",
    "        self.set_gradients = optimizer.apply_gradients(self.get_gradients)\n",
    "        \n",
    "        # variables initializer\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        \n",
    "        # define a tensorflow session\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(self.init_op)\n",
    "\n",
    "    def activation(self, x):\n",
    "        return tf.div(tf.constant(1.0), tf.add(tf.constant(1.0), tf.exp(tf.negative(x))))\n",
    "\n",
    "    def fit(self, X, Y, check_gradients=False):\n",
    "        # Apply One Hot Encoding\n",
    "        Y = onehot_encoder.transform(Y.reshape(len(Y), 1))\n",
    "        \n",
    "        costs = []\n",
    "        # Training\n",
    "        for epoch in range(self.epochs):\n",
    "            C = None\n",
    "            if check_gradients == True and epoch == (self.epochs - 1):\n",
    "                # Perform the gradient checking and optimize\n",
    "                op, grads_vars, c = self.session.run([self.set_gradients, self.get_gradients, self.cost],\n",
    "                                                     feed_dict={self.x: X, self.y: Y})\n",
    "                result = self.numeric_grad_checking(grads_vars, X, Y)\n",
    "                print(\"Numeric gradient check result: %f\" % result)\n",
    "                    \n",
    "            else:\n",
    "                # Perform the optimization alone (w/o gradient checking)\n",
    "                op, c = self.session.run([self.optimization, self.cost], feed_dict={self.x: X, self.y: Y})\n",
    "            \n",
    "            #print(\"Epoch:\", (epoch + 1), \"cost: \", c)\n",
    "            costs.append(c)\n",
    "\n",
    "        self.print_costs(costs)\n",
    "        return self\n",
    "    \n",
    "    def print_costs(self, costs):\n",
    "        plt.plot(costs)\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.show()\n",
    "\n",
    "    def numeric_grad_checking(self, grads_vars, X, Y):\n",
    "        var_list = np.asarray([v for g, v in grads_vars])\n",
    "        grad_list = np.asarray([g for g, v in grads_vars])\n",
    "        print(var_list[0].shape)\n",
    "        index = 0\n",
    "        num_grad = []\n",
    "        grads = []\n",
    "        for var in tf.trainable_variables():\n",
    "            for index_var in range(len(var_list[index])):\n",
    "                if isinstance(var_list[index][index_var],  list):\n",
    "                    for index_neuron in range(len(var_list[index][index_var])):\n",
    "                        print(\"index: %s, index_var: %s, index_neuron: %s\" % (index, index_var, index_neuron))\n",
    "                       \n",
    "                        curr_cost = self.compute_differences(var, var_list[index], var_list[index][index_var][index_neuron], X, Y)\n",
    "\n",
    "                        print(\"cost: %f, grad: %f\" % (curr_cost, grad_list[index][index_var][index_neuron]))\n",
    "                        \n",
    "                        num_grad.append(curr_cost)\n",
    "                        grads.append(grad_list[index][index_var][index_neuron])\n",
    "                else:\n",
    "                    print(\"index: %s, index_var: %s\" % (index, index_var))\n",
    "                    \n",
    "                    curr_cost = self.compute_differences(var, var_list[index], var_list[index][index_var], X, Y)\n",
    "                    \n",
    "                    print(\"cost: %f, grad: %f\" % (curr_cost, grad_list[index][index_var]))\n",
    "                        \n",
    "                    num_grad.append(curr_cost)\n",
    "                    grads.append(grad_list[index][index_var])\n",
    "                \n",
    "            # increase index\n",
    "            index += 1\n",
    "        \n",
    "        num_grad_arr = np.asarray(num_grad)\n",
    "        grads_arr = np.asarray(grads)\n",
    "        \n",
    "        return norm(grads_arr - num_grad_arr) / norm(grads_arr + num_grad_arr)\n",
    "\n",
    "    def compute_differences(self, var, variable, delta_var, X, Y):\n",
    "        e = 1e-4\n",
    "        original = delta_var\n",
    "        \n",
    "        delta_var = original + e\n",
    "        assign_add_op = var.assign(variable)\n",
    "        self.session.run(assign_add_op)\n",
    "        cost_add = self.session.run(self.cost, feed_dict={self.x: X, self.y: Y})\n",
    "        \n",
    "        delta_var = original - e\n",
    "        assign_sub_op = var.assign(variable)\n",
    "        self.session.run(assign_sub_op)\n",
    "        cost_sub = self.session.run(self.cost, feed_dict={self.x: X, self.y: Y})\n",
    "\n",
    "        # Restore original values\n",
    "        delta_var = original\n",
    "        assign_orig_op = var.assign(variable)\n",
    "        self.session.run(assign_orig_op)\n",
    "        \n",
    "        return (cost_add - cost_sub) / (2 * e)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Forward propagation\n",
    "        prediction = self.session.run(self.y_pred, feed_dict={self.x: X})\n",
    "        return np.argmax(prediction, 1)\n",
    "    \n",
    "    def close(self):\n",
    "        self.session.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Neural Network with a single hidden layer\n",
    "nn_single_hl_model = NeuralNetwork(input_size=len(data_X[0]), output_size=len(classes))\n",
    "run_kfold(lambda X, Y, check_gradients: nn_single_hl_model.fit(X, Y, check_gradients), data_Y, data_X, check_gradients=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network with two hidden layers\n",
    "nn_two_hl_model = NeuralNetwork(input_size=len(data_X[0]), output_size=len(classes), hidden_layers=[300, 300])\n",
    "run_kfold(lambda X, Y: nn_two_hl_model.fit(X, Y), data_Y, data_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
