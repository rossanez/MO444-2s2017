{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/scipy/sparse/coo.py:200: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  if np.rank(self.data) != 1 or np.rank(self.row) != 1 or np.rank(self.col) != 1:\n",
      "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/scipy/sparse/compressed.py:130: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  if np.rank(self.data) != 1 or np.rank(self.indices) != 1 or np.rank(self.indptr) != 1:\n"
     ]
    }
   ],
   "source": [
    "# MO444-A 2s/2017 - Second assignment\n",
    "#\n",
    "#         Group 05\n",
    "#\n",
    "# - Anderson Rossanez (124136)\n",
    "# - Bruno Branta Lopes (31470)\n",
    "#\n",
    "\n",
    "from __future__ import print_function\n",
    "import glob\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "from skimage.io import imread\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Some definitions\n",
    "classes = np.asarray(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "int_classes = preprocessing.LabelEncoder().fit_transform(classes) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "onehot_classes = onehot_encoder.fit_transform(int_classes.reshape(len(int_classes), 1))\n",
    "\n",
    "def load_image_dataset(name, n=None):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open('cifar-10/%s/labels' % name) as labels:\n",
    "        i = 0\n",
    "        for path in sorted(glob.glob('cifar-10/%s/*.png' % name)):\n",
    "            # Reading the images as grayscale to have a 32x32 matrix,\n",
    "            # instead of a 32x32x3 matrix in case of RGB.\n",
    "            X.append(imread(path, as_grey=True))\n",
    "            Y.append(int(labels.next()))\n",
    "            i += 1\n",
    "            if n != None and i >= n:\n",
    "                break\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load train dataset\n",
    "data_X, data_Y = load_image_dataset('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmfJ/Lf1Jv4aGfSQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116da7710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: cat\n"
     ]
    }
   ],
   "source": [
    "# Take a look at a sample and its class (207 should be a cat [3])\n",
    "plt.imshow(data_X[207], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print('Label: %s' % classes[int(data_Y[207])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape: (50000, 32, 32)\n",
      "Flattened shape: (50000, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Flatten data to 1-D\n",
    "def flatten_data(data):\n",
    "    print('Previous shape: {}'.format(data.shape))\n",
    "    reshaped = data.reshape(-1, 32*32)\n",
    "    print('Flattened shape: {}'.format(reshaped.shape))\n",
    "    return reshaped\n",
    "\n",
    "data_X = flatten_data(data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementing K-Fold to help avoiding overfitting\n",
    "from sklearn.model_selection import KFold\n",
    "from datetime import datetime\n",
    "\n",
    "random_state = np.random.RandomState(1)\n",
    "\n",
    "# prepare for 5-fold execution\n",
    "k5_fold = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "def run_kfold(method, data_Y, data_X, scale=False):\n",
    "    train_precision, train_recall, train_f1 = [], [], []\n",
    "    validation_precision, validation_recall, validation_f1 = [], [], []\n",
    "    start_time = datetime.now()\n",
    "    k = 0\n",
    "    print('k', end=' ')\n",
    "    model = None\n",
    "\n",
    "    for train_index, validation_index in k5_fold.split(data_X):\n",
    "        k += 1\n",
    "        print(k, end=' ')\n",
    "        \n",
    "        train_data_X, train_data_Y = data_X[train_index], data_Y[train_index]\n",
    "        validation_data_X, validation_data_Y = data_X[validation_index], data_Y[validation_index]\n",
    "        \n",
    "        if scale:\n",
    "            model_scaler = preprocessing.StandardScaler()\n",
    "            train_data_X = model_scaler.fit_transform(train_data_X)\n",
    "            validation_data_X = model_scaler.transform(validation_data_X)\n",
    "\n",
    "        # Train the model(s) using the training data\n",
    "        model = method(train_data_X, train_data_Y)\n",
    "        \n",
    "        # Predict training data\n",
    "        predicted_train_data_Y = model.predict(train_data_X)\n",
    "        train_precision.append(precision_score(train_data_Y, predicted_train_data_Y, average='weighted'))\n",
    "        train_recall.append(recall_score(train_data_Y, predicted_train_data_Y, average='weighted'))\n",
    "        train_f1.append(f1_score(train_data_Y, predicted_train_data_Y, average='weighted'))\n",
    "        \n",
    "        # Predict validation data\n",
    "        predicted_validation_data_Y = model.predict(validation_data_X)\n",
    "        validation_precision.append(precision_score(validation_data_Y, predicted_validation_data_Y, average='weighted'))\n",
    "        validation_recall.append(recall_score(validation_data_Y, predicted_validation_data_Y, average='weighted'))\n",
    "        validation_f1.append(f1_score(validation_data_Y, predicted_validation_data_Y, average='weighted'))\n",
    "    \n",
    "    print('time elapsed: {}\\n'.format(datetime.now() - start_time))\n",
    "    print('             Precision  Recall  F1 Score')\n",
    "    print('Training:      %5.2f    %5.2f   %5.2f' % (np.mean(train_precision), np.mean(train_recall), np.mean(train_f1)))\n",
    "    print('Validation:    %5.2f    %5.2f   %5.2f' % (np.mean(validation_precision), np.mean(validation_recall), np.mean(validation_f1)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 1 2 3 4 5 time elapsed: 0:03:23.343597\n",
      "\n",
      "             Precision  Recall  F1 Score\n",
      "Training:       0.34     0.35    0.34\n",
      "Validation:     0.27     0.27    0.27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a baseline One-vs-All logistic regression model\n",
    "ova_lr_model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=100)\n",
    "run_kfold(lambda X, Y: ova_lr_model.fit(X, Y), data_Y, data_X, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 1 2 3 4 5 time elapsed: 0:01:01.747849\n",
      "\n",
      "             Precision  Recall  F1 Score\n",
      "Training:       0.34     0.34    0.34\n",
      "Validation:     0.27     0.28    0.27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Multinomial (Softmax) logistic regression model\n",
    "mn_lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100)\n",
    "run_kfold(lambda X, Y: mn_lr_model.fit(X, Y), data_Y, data_X, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Networks\n",
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, learning_rate=0.5, epochs=10, batch_size=100):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.epochs=epochs\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        # input data placeholders\n",
    "        self.x = tf.placeholder(tf.float32, [None, input_size])\n",
    "        # output data placeholder\n",
    "        self.y = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "        # now declare the weights connecting the input to the hidden layer\n",
    "        W1 = tf.Variable(tf.random_normal([input_size, 300], stddev=0.03), name='W1')\n",
    "        b1 = tf.Variable(tf.random_normal([300]), name='b1')\n",
    "\n",
    "        # and the weights connecting the hidden layer to the output layer\n",
    "        W2 = tf.Variable(tf.random_normal([300, output_size], stddev=0.03), name='W2')\n",
    "        b2 = tf.Variable(tf.random_normal([output_size]), name='b2')\n",
    "        \n",
    "        # hidden layer's output\n",
    "        Z1 = tf.add(tf.matmul(self.x, W1), b1)\n",
    "        a1 = tf.nn.relu(Z1)\n",
    "        \n",
    "        # output with softmax activation\n",
    "        Z2 = tf.add(tf.matmul(a1, W2), b2)\n",
    "        self.y_ = tf.nn.softmax(Z2)\n",
    "        \n",
    "        y_clipped = tf.clip_by_value(self.y_, 1e-10, 0.9999999)\n",
    "        self.cross_entropy = -tf.reduce_mean(tf.reduce_sum(self.y * tf.log(y_clipped) + (1 - self.y) * tf.log(1 - y_clipped), axis=1))\n",
    "        # add an optimizer\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.cross_entropy)\n",
    "        \n",
    "        # finally setup the initialisation operator\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        \n",
    "        # define an accuracy assessment operation\n",
    "        correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(self.init_op)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # Apply One Hot Encoding\n",
    "        Y = onehot_encoder.transform(Y.reshape(len(Y), 1))\n",
    "        \n",
    "        # start the session\n",
    "        for epoch in range(self.epochs):\n",
    "            _, c = self.session.run([self.optimizer, self.cross_entropy], feed_dict={self.x: X, self.y: Y})\n",
    "            print(\"Epoch:\", (epoch + 1))\n",
    "        print(self.session.run(self.accuracy, feed_dict={self.x: X, self.y: Y}))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward propagation\n",
    "        return self.session.run(self.y_, feed_dict={self.x: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "0.1\n",
      "[[  5.58083099e-37   0.00000000e+00   5.39958293e-29   0.00000000e+00\n",
      "    1.25233580e-29   1.09575393e-29   1.00000000e+00   1.46335195e-30\n",
      "    0.00000000e+00   3.59819875e-23]\n",
      " [  0.00000000e+00   0.00000000e+00   2.02655887e-33   0.00000000e+00\n",
      "    4.28415574e-34   3.16188006e-34   1.00000000e+00   3.68308381e-35\n",
      "    0.00000000e+00   1.19920848e-26]]\n"
     ]
    }
   ],
   "source": [
    "# Neural Network with a single hidden layer\n",
    "nn_single_hl_model = NeuralNetwork(input_size=len(data_X[0]), output_size=len(classes))\n",
    "#run_kfold(lambda X, Y: nn_single_hl_model.fit(X, Y), data_Y, data_X, scale=True)\n",
    "nn_single_hl_model.fit(data_X, data_Y)\n",
    "print(nn_single_hl_model.predict(data_X[0:2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
